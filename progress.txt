11/17
Starting to note my progress

Finished first official benchmarks on set of prediction data.

Compared prediction to a list of addresses, compared prediction adress to the next actual adress


//////////////NormModel/////////////////
Hit Count: 28
Miss Count: 69
Hit Rate: 28.865979381443296
///////////////////////////////

Since the hit rate seemed relatiely low, I trained on a larger set of data, NormModel2.
The following data is listed as the following:

//////////NormModel2///////////
Hit Count: 25
Miss Count: 72
Hit Rate: 25.773195876288657
///////////////////////////////

Data ^^ is null since we only comapred the prediction to every possible actual address
instead of comparing the NEXT actual address to predictions that have been prefetched

This disproves the concept that more data == better model. 
Could be a case of overtraining.


New Data: Compare actual address [i+6] to prediction set

//////////NormModel////////////
Hit Count: 17
Miss Count: 78
Hit Rate: 17.894736842105264
///////////////////////////////

Lower than expected

//////////NormModel2///////////
Hit Count: 40
Miss Count: 55
Hit Rate: 42.10526315789473
///////////////////////////////

A lot higher than expected

Above claim in line 27 is wrong. The more thoroughly trained model made a difference of ~25%

hw:
    try to find what happens when there is a huge range in LSTMs
    Have something that discards outliers within a set of data (min, max), average, etc
      find middle point of min and max and average of data


1/31
Starting again after winter break

Page and Offset model application
 
page ex. 0xBEEF >> 12 -> 0xB 
offset ex. (0xBEEF) & (0xFFFFFFFFFFFFFFFF) -> 0xEEF

Each address is 32 bytes

//// pause on page and offset models
//// Start on embedding layer of model based on voyager paper
voyager github repo has a embed layer already made, hack some code and get it in
***get the embed layer done by the end of the month

2/1
Attempts at integrating embed layer into trainModel and use within main prediction function
Error 1: Output of the prediction was >1 array instead of one ouput, likely due to embedding turning single values into vector addresses, moved embed layer to before the LSTM
Error 2: Graph execution error


2/7 - 2/16
Worked on new neural network, adapted by Dr. Tuck from a repository that also attempted to recreate Voyager. A few issues arose that were ultimately dealt with my Dr. Tuck.
Spent time discussing next steps, future plans, and overall goal of the research project
There is now a working base model that we can freely change and use to find data

2nd Semester Timeline:

A) Things we can do to collect data and expand our understanding. 1 month

1. Improve the accuracy of our prefetching/cache metrics. 
2. Add more traces and evaluate how well they work.
3. Consider different model designs and how they affect prefetcher metrics. Examples: unifying page+offset, no MHA, smaller LSTM layer, generic RNN, remove embedding layer.   
4. Evaluate how size of model (# of weights) influences accuracy of prefetcher.

B) Finalize data to collect. 1-2 weeks.
C) Collect and present final set of data in charts/graphs to explain what it means. 1-2 weeks.
D) Make poster and write-up final report. 

2/21
Worked on simple cache and implemented into main
There is now a cache of predicted values that the prefetcher checks with and a cache of accesses to measure accuracy of the prefetcher
All changes have been comitted to main

3/7 - 3/16: Somehow all of my progress that I wrote into this file didn't save for the last two weeks 
so I will be recalling what I have been doing over this time period by memory.

From 3/7 to 3/9, I spent time reviewing papers and previous repositories as I waited for the models to finish training.

From 3/14 to 3/16, wrote in accurate parameters of the prefetcher to be accurate to the Voyager paper. Fixed a few errors with embedding and
keys. Basically just set the addresses that don't exist in the model's database to 0 and don't use it to call for a prefetch.

3/21: Reviewed prefetcher performance, it was weirdly good. Dr. Tuck pointed out the flawed design of the data collection since the predict cache
only compared prefecthed values to each other instead of comparing incoming addresses to prefetched addresses. Fixed this issue.

3/23: Seems to be an issue with the prefetch output of model 0 since it's only giving one to two digit numbers. The other two models does not suffer 
from this problem as much.

3/28
Started training model, figured out root of embed and key issues: I haven't been saving the dictionaries of each individual model to separate files 
so models 0 and 1 were missing their dictionaries. Training on segment 1 to prefetch segement 2 with saved dictionaries. Found that access cache on 
bfs achieve high access rates, changing to pagerank: plan to train on a pagerank trace to prefetch since bfs has good cache performance, skewing the
prefetcher/access stream.

